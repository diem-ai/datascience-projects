{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Introduction: </h4>\n",
    "<p> Project aims to classify messages spam/not spam using <b> word2vec model trained by spam.csv </b> and  supervised machine learning algorithms such as SVC, LogisticRegression and KNearestNeighbor</p>\n",
    "<p> Finally, compute and compare the accuracy score of three models </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Import Libraries </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\btdiem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\btdiem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\btdiem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\btdiem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ignore warning come out from functions\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\"\n",
    "Data tools\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "\"\"\"\n",
    "NLP tools\n",
    "\"\"\"\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.models\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\"\"\"\n",
    "supervised classifiers\n",
    "\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"\"\"\n",
    "Libraries for accessory functions\n",
    "\"\"\"\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\"\"\"\n",
    "Plot result\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Accessory Functions </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pickle(file_name, data):      \n",
    "    \"\"\"\n",
    "    Function writes serialised object\n",
    "    \n",
    "    Arg:\n",
    "     - file_name: path + file name. for example: data/google_model.pkl\n",
    "     - data: pickle object\n",
    "    \"\"\"\n",
    "    output = open(file_name, 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(data, output)\n",
    "    \n",
    "    output.close()\n",
    "\n",
    "def read_pickle(file_name):\n",
    "    \"\"\"\n",
    "    Function reads pickle file\n",
    "    \n",
    "    Arg:\n",
    "        - file_name: path + pickle file name , for example:C:/data/google_model.pkl\n",
    "    \"\"\"\n",
    "    pkl_file = open(file_name, 'rb')\n",
    "\n",
    "    google_model = pickle.load(pkl_file)\n",
    "\n",
    "    pkl_file.close()\n",
    "    \n",
    "\n",
    "def tokenizer(w2vec_model, doc):\n",
    "    \"\"\"\n",
    "    Function tokenizes doc and return vector of words found in w2vec_model\n",
    "    \n",
    "    Arg:\n",
    "        - w2vec_model: a gensim word2vec model\n",
    "        - doc: a string, for example: 'Using NLP to gain insights from employee review data'\n",
    "    \n",
    "    Return:\n",
    "        A list of words in w2vec_model\n",
    "    \n",
    "    \"\"\"\n",
    "    vector = []\n",
    "    tokens = doc.split(' ')\n",
    "    for w in tokens:\n",
    "        if w in w2vec_model.vocab.keys():\n",
    "            vector.append(w)\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def get_doc_vec(words, w2vec_model):\n",
    "    \"\"\"\n",
    "    Function to take a document as a list of words and return the document vector\n",
    "    \n",
    "    Arg:\n",
    "        - words: a list of words, for example: [\"w1\", \"w2\", \"w3\"]\n",
    "        - w2vec_model: vector of vocabularies\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    good_words = []\n",
    "    for word in words:\n",
    "        # Words not in the original model will fair\n",
    "        try:\n",
    "            if word in w2vec_model:\n",
    "                good_words.append(word)\n",
    "        except:\n",
    "            continue\n",
    "    # If no words are in the original model\n",
    "    # print(\"good_words: {}\".format(good_words))\n",
    "    if len(good_words) == 0:\n",
    "        return None\n",
    "    # Return the mean of the vectors for all the good words\n",
    "    return w2vec_model[good_words].mean(axis=0)\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert the part-of-speech naming scheme\n",
    "    from the nltk default to that which is\n",
    "    recognized by the WordNet lemmatizer\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    \n",
    "def preprocess_series_text(data):\n",
    "    \"\"\"\n",
    "    Perform complete preprocessing on a Pandas series\n",
    "    including removal of alpha numerical words, normalization,\n",
    "    punctuation removal, tokenization, stop word removal, \n",
    "    and lemmatization.\n",
    "       \n",
    "    Arg:\n",
    "        data: pandas series \n",
    "    \n",
    "    Return:\n",
    "        processed pandas series\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove alpha numerical words and make lowercase\n",
    "    alphanum_re = re.compile(r\"\"\"\\w*\\d\\w*\"\"\")\n",
    "    alphanum_lambda = lambda x: alphanum_re.sub('', x.strip().lower())\n",
    "\n",
    "    data = data.map(alphanum_lambda)\n",
    "\n",
    "    # remove punctuation\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    punc_lambda = lambda x: punc_re.sub(' ', x)\n",
    "\n",
    "    data = data.map(punc_lambda)\n",
    "\n",
    "    # tokenize words\n",
    "    data = data.map(word_tokenize)\n",
    "\n",
    "    # remove stop words\n",
    "    sw = stopwords.words('english')\n",
    "    sw_lambda = lambda x: list(filter(lambda y: y not in sw, x))\n",
    "\n",
    "    data = data.map(sw_lambda)\n",
    "\n",
    "    # part of speech tagging--must convert to format used by lemmatizer\n",
    "    data = data.map(nltk.pos_tag)\n",
    "    pos_lambda = lambda x: [(y[0], get_wordnet_pos(y[1])) for y in x]\n",
    "    data = data.map(pos_lambda)\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_lambda = lambda x: [lemmatizer.lemmatize(*y) for y in x]\n",
    "    data = data.map(lem_lambda)\n",
    "    \n",
    "    return data.map(' '.join)\n",
    "\n",
    "def get_mean_score(clf, cv=10):\n",
    "    \"\"\"\n",
    "    Get the scores from classifier and return the mean value\n",
    "    \n",
    "    Arg:\n",
    "     - clf: classifier, for ex: LogisticRegression, SVC,..\n",
    "     - cv : cross validation parameter\n",
    "     \n",
    "    Return:\n",
    "        mean of scores\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(clf, X, y , cv=10)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Word2Vec Vectors Model with spam.csv: \n",
    "* Read the spam dataset into a Dataframe.\n",
    "* Preprocess the documents using <code> preprocess_series_text(data) </code>.\n",
    "* Split each document into a list of words.\n",
    "* Use the results to train your own word2vec model with `gensim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read spam_csv and print out first 5 rows\n",
    "\n",
    "spam_data = pd.read_csv('data/spam.csv', sep='\\t', \n",
    "                        header=None, names=[\"label\", \"text\"])                         \n",
    "\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "\n",
    "spam_data['text'] = preprocess_series_text(spam_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, jurong, point, crazy, available, bugis, n...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
       "3        [u, dun, say, early, hor, u, c, already, say]\n",
       "4          [nah, think, go, usf, life, around, though]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each document into a list of words.\n",
    "sentences = spam_data['text'].str.split()\n",
    "\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec with spam data\n",
    "spam_w2vec_model = Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Check out some conceptual comparisons with our word2vec model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('free', 0.9998507499694824),\n",
       " ('mobile', 0.9998424053192139),\n",
       " ('number', 0.9998326897621155),\n",
       " ('line', 0.9997907876968384),\n",
       " ('voucher', 0.9997876286506653),\n",
       " ('week', 0.9997807741165161),\n",
       " ('customer', 0.9997776746749878),\n",
       " ('text', 0.9997731447219849),\n",
       " ('receive', 0.9997721910476685),\n",
       " ('urgent', 0.9997670650482178)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_w2vec_model.most_similar('call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('holiday', 0.9999002814292908),\n",
       " ('voucher', 0.9998716115951538),\n",
       " ('ur', 0.9998674392700195),\n",
       " ('c', 0.9998595118522644),\n",
       " ('word', 0.9998503923416138),\n",
       " ('week', 0.999850332736969),\n",
       " ('collect', 0.9998490810394287),\n",
       " ('msg', 0.9998487234115601),\n",
       " ('play', 0.9998477101325989),\n",
       " ('per', 0.9998436570167542)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_w2vec_model.most_similar('win')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> From Word2Vec we have vectors for words,  How do we get vectors for whole documents? </p>\n",
    "<p> We have to make vector of document from word vector</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.1444864, 0.25257498, 0.05145365, -0.0599798...\n",
       "1    [0.15285298, 0.2741392, 0.057334818, -0.064060...\n",
       "2    [0.13937083, 0.23683971, 0.04580003, -0.052127...\n",
       "3    [0.1979268, 0.35462674, 0.074506976, -0.083067...\n",
       "4    [0.15175353, 0.2638947, 0.053968746, -0.061849...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_vecs = spam_data.text.str.split().map(lambda x: get_doc_vec(x, spam_w2vec_model))\n",
    "spam_vecs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Add <code> spam_vecs </code> as new feature to <code> spam_data </code> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>vecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>[0.1444864, 0.25257498, 0.05145365, -0.0599798...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[0.15285298, 0.2741392, 0.057334818, -0.064060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts may...</td>\n",
       "      <td>[0.13937083, 0.23683971, 0.04580003, -0.052127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>[0.1979268, 0.35462674, 0.074506976, -0.083067...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "      <td>[0.15175353, 0.2638947, 0.053968746, -0.061849...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  go jurong point crazy available bugis n great ...   \n",
       "1   ham                            ok lar joking wif u oni   \n",
       "2  spam  free entry wkly comp win fa cup final tkts may...   \n",
       "3   ham                u dun say early hor u c already say   \n",
       "4   ham                nah think go usf life around though   \n",
       "\n",
       "                                                vecs  \n",
       "0  [0.1444864, 0.25257498, 0.05145365, -0.0599798...  \n",
       "1  [0.15285298, 0.2741392, 0.057334818, -0.064060...  \n",
       "2  [0.13937083, 0.23683971, 0.04580003, -0.052127...  \n",
       "3  [0.1979268, 0.35462674, 0.074506976, -0.083067...  \n",
       "4  [0.15175353, 0.2638947, 0.053968746, -0.061849...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['vecs'] = spam_vecs\n",
    "\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Some of the documents have no good words in them. Let's drop them from our dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5517, 3)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = spam_data.dropna()\n",
    "\n",
    "spam_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> What is the length of vector ?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_data.vecs[:1].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>0.144486</td>\n",
       "      <td>0.252575</td>\n",
       "      <td>0.051454</td>\n",
       "      <td>-0.059980</td>\n",
       "      <td>-0.033501</td>\n",
       "      <td>-0.069658</td>\n",
       "      <td>0.217711</td>\n",
       "      <td>-0.395451</td>\n",
       "      <td>0.125066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305336</td>\n",
       "      <td>0.243675</td>\n",
       "      <td>-0.046680</td>\n",
       "      <td>0.089231</td>\n",
       "      <td>-0.178807</td>\n",
       "      <td>-0.211533</td>\n",
       "      <td>-0.128441</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.055130</td>\n",
       "      <td>-0.654755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>0.152853</td>\n",
       "      <td>0.274139</td>\n",
       "      <td>0.057335</td>\n",
       "      <td>-0.064060</td>\n",
       "      <td>-0.034944</td>\n",
       "      <td>-0.077309</td>\n",
       "      <td>0.235172</td>\n",
       "      <td>-0.432041</td>\n",
       "      <td>0.138651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.263969</td>\n",
       "      <td>-0.052178</td>\n",
       "      <td>0.097975</td>\n",
       "      <td>-0.193324</td>\n",
       "      <td>-0.231282</td>\n",
       "      <td>-0.137487</td>\n",
       "      <td>0.037318</td>\n",
       "      <td>0.060486</td>\n",
       "      <td>-0.711483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>0.139371</td>\n",
       "      <td>0.236840</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>-0.052128</td>\n",
       "      <td>-0.034396</td>\n",
       "      <td>-0.060149</td>\n",
       "      <td>0.208590</td>\n",
       "      <td>-0.376427</td>\n",
       "      <td>0.119853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287832</td>\n",
       "      <td>0.222790</td>\n",
       "      <td>-0.046136</td>\n",
       "      <td>0.083759</td>\n",
       "      <td>-0.172661</td>\n",
       "      <td>-0.200625</td>\n",
       "      <td>-0.125590</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>-0.632499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>0.197927</td>\n",
       "      <td>0.354627</td>\n",
       "      <td>0.074507</td>\n",
       "      <td>-0.083068</td>\n",
       "      <td>-0.045381</td>\n",
       "      <td>-0.103098</td>\n",
       "      <td>0.300230</td>\n",
       "      <td>-0.553187</td>\n",
       "      <td>0.175941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424584</td>\n",
       "      <td>0.334762</td>\n",
       "      <td>-0.063873</td>\n",
       "      <td>0.124478</td>\n",
       "      <td>-0.249485</td>\n",
       "      <td>-0.299436</td>\n",
       "      <td>-0.179657</td>\n",
       "      <td>0.046965</td>\n",
       "      <td>0.075864</td>\n",
       "      <td>-0.915028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>0.151754</td>\n",
       "      <td>0.263895</td>\n",
       "      <td>0.053969</td>\n",
       "      <td>-0.061849</td>\n",
       "      <td>-0.033135</td>\n",
       "      <td>-0.076426</td>\n",
       "      <td>0.227409</td>\n",
       "      <td>-0.410801</td>\n",
       "      <td>0.132604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315791</td>\n",
       "      <td>0.255154</td>\n",
       "      <td>-0.049716</td>\n",
       "      <td>0.092760</td>\n",
       "      <td>-0.186815</td>\n",
       "      <td>-0.221754</td>\n",
       "      <td>-0.133469</td>\n",
       "      <td>0.034608</td>\n",
       "      <td>0.058734</td>\n",
       "      <td>-0.681842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label         0         1         2         3         4         5         6  \\\n",
       "0   ham  0.144486  0.252575  0.051454 -0.059980 -0.033501 -0.069658  0.217711   \n",
       "1   ham  0.152853  0.274139  0.057335 -0.064060 -0.034944 -0.077309  0.235172   \n",
       "2  spam  0.139371  0.236840  0.045800 -0.052128 -0.034396 -0.060149  0.208590   \n",
       "3   ham  0.197927  0.354627  0.074507 -0.083068 -0.045381 -0.103098  0.300230   \n",
       "4   ham  0.151754  0.263895  0.053969 -0.061849 -0.033135 -0.076426  0.227409   \n",
       "\n",
       "          7         8  ...        90        91        92        93        94  \\\n",
       "0 -0.395451  0.125066  ...  0.305336  0.243675 -0.046680  0.089231 -0.178807   \n",
       "1 -0.432041  0.138651  ...  0.330311  0.263969 -0.052178  0.097975 -0.193324   \n",
       "2 -0.376427  0.119853  ...  0.287832  0.222790 -0.046136  0.083759 -0.172661   \n",
       "3 -0.553187  0.175941  ...  0.424584  0.334762 -0.063873  0.124478 -0.249485   \n",
       "4 -0.410801  0.132604  ...  0.315791  0.255154 -0.049716  0.092760 -0.186815   \n",
       "\n",
       "         95        96        97        98        99  \n",
       "0 -0.211533 -0.128441  0.034884  0.055130 -0.654755  \n",
       "1 -0.231282 -0.137487  0.037318  0.060486 -0.711483  \n",
       "2 -0.200625 -0.125590  0.027405  0.050847 -0.632499  \n",
       "3 -0.299436 -0.179657  0.046965  0.075864 -0.915028  \n",
       "4 -0.221754 -0.133469  0.034608  0.058734 -0.681842  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lenght of vector\n",
    "len_vec = len(spam_data.vecs[:1].values[0])\n",
    "# Create a Numpy array of the document vectors\n",
    "spam_np_vecs = np.zeros((len(spam_data), len_vec))\n",
    "for i, vec in enumerate(spam_data.vecs):\n",
    "    spam_np_vecs[i, :] = vec\n",
    "    \n",
    "# Combine the full dataframe with the labels\n",
    "spam_w2v_data = pd.concat([spam_data.reset_index().label, pd.DataFrame(spam_np_vecs)], axis=1)\n",
    "\n",
    "spam_w2v_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Everything seems familiar now. We have training data and label data</p>\n",
    "<p> We shall transform label column to numeric values <code> (1: spam, 0: ham) </code> so that machine can understand </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.144486</td>\n",
       "      <td>0.252575</td>\n",
       "      <td>0.051454</td>\n",
       "      <td>-0.059980</td>\n",
       "      <td>-0.033501</td>\n",
       "      <td>-0.069658</td>\n",
       "      <td>0.217711</td>\n",
       "      <td>-0.395451</td>\n",
       "      <td>0.125066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305336</td>\n",
       "      <td>0.243675</td>\n",
       "      <td>-0.046680</td>\n",
       "      <td>0.089231</td>\n",
       "      <td>-0.178807</td>\n",
       "      <td>-0.211533</td>\n",
       "      <td>-0.128441</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.055130</td>\n",
       "      <td>-0.654755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.152853</td>\n",
       "      <td>0.274139</td>\n",
       "      <td>0.057335</td>\n",
       "      <td>-0.064060</td>\n",
       "      <td>-0.034944</td>\n",
       "      <td>-0.077309</td>\n",
       "      <td>0.235172</td>\n",
       "      <td>-0.432041</td>\n",
       "      <td>0.138651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.263969</td>\n",
       "      <td>-0.052178</td>\n",
       "      <td>0.097975</td>\n",
       "      <td>-0.193324</td>\n",
       "      <td>-0.231282</td>\n",
       "      <td>-0.137487</td>\n",
       "      <td>0.037318</td>\n",
       "      <td>0.060486</td>\n",
       "      <td>-0.711483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.139371</td>\n",
       "      <td>0.236840</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>-0.052128</td>\n",
       "      <td>-0.034396</td>\n",
       "      <td>-0.060149</td>\n",
       "      <td>0.208590</td>\n",
       "      <td>-0.376427</td>\n",
       "      <td>0.119853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287832</td>\n",
       "      <td>0.222790</td>\n",
       "      <td>-0.046136</td>\n",
       "      <td>0.083759</td>\n",
       "      <td>-0.172661</td>\n",
       "      <td>-0.200625</td>\n",
       "      <td>-0.125590</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>-0.632499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.197927</td>\n",
       "      <td>0.354627</td>\n",
       "      <td>0.074507</td>\n",
       "      <td>-0.083068</td>\n",
       "      <td>-0.045381</td>\n",
       "      <td>-0.103098</td>\n",
       "      <td>0.300230</td>\n",
       "      <td>-0.553187</td>\n",
       "      <td>0.175941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424584</td>\n",
       "      <td>0.334762</td>\n",
       "      <td>-0.063873</td>\n",
       "      <td>0.124478</td>\n",
       "      <td>-0.249485</td>\n",
       "      <td>-0.299436</td>\n",
       "      <td>-0.179657</td>\n",
       "      <td>0.046965</td>\n",
       "      <td>0.075864</td>\n",
       "      <td>-0.915028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.151754</td>\n",
       "      <td>0.263895</td>\n",
       "      <td>0.053969</td>\n",
       "      <td>-0.061849</td>\n",
       "      <td>-0.033135</td>\n",
       "      <td>-0.076426</td>\n",
       "      <td>0.227409</td>\n",
       "      <td>-0.410801</td>\n",
       "      <td>0.132604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315791</td>\n",
       "      <td>0.255154</td>\n",
       "      <td>-0.049716</td>\n",
       "      <td>0.092760</td>\n",
       "      <td>-0.186815</td>\n",
       "      <td>-0.221754</td>\n",
       "      <td>-0.133469</td>\n",
       "      <td>0.034608</td>\n",
       "      <td>0.058734</td>\n",
       "      <td>-0.681842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         0         1         2         3         4         5  \\\n",
       "0      0  0.144486  0.252575  0.051454 -0.059980 -0.033501 -0.069658   \n",
       "1      0  0.152853  0.274139  0.057335 -0.064060 -0.034944 -0.077309   \n",
       "2      1  0.139371  0.236840  0.045800 -0.052128 -0.034396 -0.060149   \n",
       "3      0  0.197927  0.354627  0.074507 -0.083068 -0.045381 -0.103098   \n",
       "4      0  0.151754  0.263895  0.053969 -0.061849 -0.033135 -0.076426   \n",
       "\n",
       "          6         7         8  ...        90        91        92        93  \\\n",
       "0  0.217711 -0.395451  0.125066  ...  0.305336  0.243675 -0.046680  0.089231   \n",
       "1  0.235172 -0.432041  0.138651  ...  0.330311  0.263969 -0.052178  0.097975   \n",
       "2  0.208590 -0.376427  0.119853  ...  0.287832  0.222790 -0.046136  0.083759   \n",
       "3  0.300230 -0.553187  0.175941  ...  0.424584  0.334762 -0.063873  0.124478   \n",
       "4  0.227409 -0.410801  0.132604  ...  0.315791  0.255154 -0.049716  0.092760   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0 -0.178807 -0.211533 -0.128441  0.034884  0.055130 -0.654755  \n",
       "1 -0.193324 -0.231282 -0.137487  0.037318  0.060486 -0.711483  \n",
       "2 -0.172661 -0.200625 -0.125590  0.027405  0.050847 -0.632499  \n",
       "3 -0.249485 -0.299436 -0.179657  0.046965  0.075864 -0.915028  \n",
       "4 -0.186815 -0.221754 -0.133469  0.034608  0.058734 -0.681842  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_w2v_data['label'] = np.where(spam_w2v_data['label']=='spam', 1, 0)\n",
    "\n",
    "spam_w2v_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Spam Classification with Word2Vec </h4>\n",
    "<p>Try with LogisticRegression, KNeighborsClassifier (n_neighbors=100) and SVC(C=100000) with cross validation cv=10</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = spam_w2v_data['label']\n",
    "\n",
    "X = spam_w2v_data.drop(['label'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LogisticRegression  KNeighbors      SVC\n",
      "0            0.958857    0.899401  0.96701\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = {}\n",
    "\n",
    "classifiers = {\"LogisticRegression\" : LogisticRegression(C=1000000)\n",
    "              , \"KNeighbors\" : KNeighborsClassifier(n_neighbors=100)\n",
    "              , 'SVC' : SVC(C=100000)}\n",
    "\n",
    "p_cv = 10\n",
    "\n",
    "for (clf_name, clf) in classifiers.items():\n",
    "    score =  get_mean_score(clf, cv = p_cv)\n",
    "    result.update({clf_name: [score]})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "print(df)\n",
    "\n",
    "df.plot(kind='bar', figsize=(8, 5))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Conclusion:</b> It seems that Support Vector Machine performs the best among three. LogisticRegression takes the second position and KNeighborsClassifier comes the last </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
